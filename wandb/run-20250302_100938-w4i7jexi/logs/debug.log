2025-03-02 10:09:38,961 INFO    MainThread:360778 [wandb_setup.py:_flush():67] Current SDK version is 0.19.7
2025-03-02 10:09:38,961 INFO    MainThread:360778 [wandb_setup.py:_flush():67] Configure stats pid to 360778
2025-03-02 10:09:38,961 INFO    MainThread:360778 [wandb_setup.py:_flush():67] Loading settings from /root/.config/wandb/settings
2025-03-02 10:09:38,961 INFO    MainThread:360778 [wandb_setup.py:_flush():67] Loading settings from /mnt/data/Logic-RL-logic/wandb/settings
2025-03-02 10:09:38,961 INFO    MainThread:360778 [wandb_setup.py:_flush():67] Loading settings from environment variables
2025-03-02 10:09:38,961 INFO    MainThread:360778 [wandb_init.py:setup_run_log_directory():647] Logging user logs to /mnt/data/Logic-RL-logic/wandb/run-20250302_100938-w4i7jexi/logs/debug.log
2025-03-02 10:09:38,962 INFO    MainThread:360778 [wandb_init.py:setup_run_log_directory():648] Logging internal logs to /mnt/data/Logic-RL-logic/wandb/run-20250302_100938-w4i7jexi/logs/debug-internal.log
2025-03-02 10:09:38,962 INFO    MainThread:360778 [wandb_init.py:init():761] calling init triggers
2025-03-02 10:09:38,962 INFO    MainThread:360778 [wandb_init.py:init():766] wandb.init called with sweep_config: {}
config: {'data': {'tokenizer': None, 'train_files': './data/formula/1/train.parquet', 'val_files': './data/formula/3/test.parquet', 'prompt_key': 'prompt', 'max_prompt_length': 512, 'max_response_length': 4096, 'train_batch_size': 8, 'val_batch_size': 8, 'return_raw_input_ids': False, 'return_raw_chat': False}, 'actor_rollout_ref': {'hybrid_engine': True, 'model': {'path': 'Qwen/Qwen2.5-3B-Instruct', 'external_lib': None, 'override_config': {}, 'enable_gradient_checkpointing': True, 'use_remove_padding': True}, 'actor': {'strategy': 'fsdp', 'ppo_mini_batch_size': 256, 'ppo_micro_batch_size': 64, 'use_dynamic_bsz': False, 'ppo_max_token_len_per_gpu': 16384, 'grad_clip': 1.0, 'clip_ratio': 0.2, 'entropy_coeff': 0.001, 'use_kl_loss': True, 'kl_loss_coef': 0.001, 'kl_loss_type': 'low_var_kl', 'ppo_epochs': 1, 'shuffle': False, 'ulysses_sequence_parallel_size': 1, 'optim': {'lr': 5e-07, 'lr_warmup_steps_ratio': 0.0, 'min_lr_ratio': None, 'warmup_style': 'constant', 'total_training_steps': 560}, 'fsdp_config': {'wrap_policy': {'min_num_params': 0}, 'param_offload': False, 'grad_offload': False, 'optimizer_offload': False, 'fsdp_size': -1}}, 'ref': {'fsdp_config': {'param_offload': True, 'wrap_policy': {'min_num_params': 0}, 'fsdp_size': -1}, 'log_prob_micro_batch_size': 160, 'log_prob_use_dynamic_bsz': False, 'log_prob_max_token_len_per_gpu': 16384, 'ulysses_sequence_parallel_size': 1}, 'rollout': {'name': 'vllm', 'temperature': 0.7, 'top_k': -1, 'top_p': 1, 'prompt_length': 512, 'response_length': 4096, 'dtype': 'bfloat16', 'gpu_memory_utilization': 0.6, 'ignore_eos': False, 'enforce_eager': True, 'free_cache_engine': True, 'load_format': 'dummy_dtensor', 'tensor_model_parallel_size': 2, 'max_num_batched_tokens': 8192, 'max_num_seqs': 1024, 'log_prob_micro_batch_size': 160, 'log_prob_use_dynamic_bsz': False, 'log_prob_max_token_len_per_gpu': 16384, 'do_sample': True, 'n': 8}}, 'critic': {'strategy': 'fsdp', 'optim': {'lr': 1e-05, 'lr_warmup_steps_ratio': 0.0, 'min_lr_ratio': None, 'warmup_style': 'constant', 'total_training_steps': 560}, 'model': {'path': '~/models/deepseek-llm-7b-chat', 'tokenizer_path': 'Qwen/Qwen2.5-3B-Instruct', 'override_config': {}, 'external_lib': None, 'enable_gradient_checkpointing': False, 'use_remove_padding': False, 'fsdp_config': {'param_offload': False, 'grad_offload': False, 'optimizer_offload': False, 'wrap_policy': {'min_num_params': 0}, 'fsdp_size': -1}}, 'ppo_mini_batch_size': 256, 'ppo_micro_batch_size': 64, 'forward_micro_batch_size': 64, 'use_dynamic_bsz': False, 'ppo_max_token_len_per_gpu': 32768, 'forward_max_token_len_per_gpu': 32768, 'ulysses_sequence_parallel_size': 1, 'ppo_epochs': 1, 'shuffle': False, 'grad_clip': 1.0, 'cliprange_value': 0.5}, 'reward_model': {'enable': False, 'strategy': 'fsdp', 'model': {'input_tokenizer': 'Qwen/Qwen2.5-3B-Instruct', 'path': '~/models/FsfairX-LLaMA3-RM-v0.1', 'external_lib': None, 'use_remove_padding': False, 'fsdp_config': {'min_num_params': 0, 'param_offload': False}}, 'micro_batch_size': 64, 'max_length': None, 'ulysses_sequence_parallel_size': 1, 'use_dynamic_bsz': False, 'forward_max_token_len_per_gpu': 32768}, 'algorithm': {'gamma': 1.0, 'lam': 1.0, 'adv_estimator': 'reinforce_plus_plus', 'kl_penalty': 'kl', 'kl_ctrl': {'type': 'fixed', 'kl_coef': 0.001}}, 'trainer': {'total_epochs': 5, 'total_training_steps': None, 'project_name': 'meta-reasoning', 'experiment_name': 'RF++-Qwen-7B-1M-xppl-curriculum-logic', 'logger': ['console', 'wandb'], 'nnodes': 1, 'n_gpus_per_node': 8, 'save_freq': 60, 'test_freq': 10, 'critic_warmup': 0, 'default_hdfs_dir': '~/experiments/gsm8k/ppo/RF++-Qwen-7B-1M-xppl-curriculum-logic', 'default_local_dir': 'checkpoints/meta-reasoning/RF++-Qwen-7B-1M-xppl-curriculum-logic'}, '_wandb': {}}
2025-03-02 10:09:38,962 INFO    MainThread:360778 [wandb_init.py:init():784] starting backend
2025-03-02 10:09:38,962 INFO    MainThread:360778 [wandb_init.py:init():788] sending inform_init request
2025-03-02 10:09:38,964 INFO    MainThread:360778 [backend.py:_multiprocessing_setup():97] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2025-03-02 10:09:38,964 INFO    MainThread:360778 [wandb_init.py:init():803] backend started and connected
2025-03-02 10:09:38,967 INFO    MainThread:360778 [wandb_init.py:init():896] updated telemetry
2025-03-02 10:09:38,972 INFO    MainThread:360778 [wandb_init.py:init():920] communicating run to backend with 90.0 second timeout
2025-03-02 10:09:39,400 INFO    MainThread:360778 [wandb_init.py:init():995] starting run threads in backend
2025-03-02 10:09:39,590 INFO    MainThread:360778 [wandb_run.py:_console_start():2377] atexit reg
2025-03-02 10:09:39,591 INFO    MainThread:360778 [wandb_run.py:_redirect():2227] redirect: wrap_raw
2025-03-02 10:09:39,591 INFO    MainThread:360778 [wandb_run.py:_redirect():2292] Wrapping output streams.
2025-03-02 10:09:39,591 INFO    MainThread:360778 [wandb_run.py:_redirect():2317] Redirects installed.
2025-03-02 10:09:39,593 INFO    MainThread:360778 [wandb_init.py:init():1037] run started, returning control to user process
2025-03-02 10:11:32,709 WARNING MsgRouterThr:360778 [router.py:message_loop():77] message_loop has been closed
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/wandb/sdk/interface/router_sock.py", line 28, in _read_message
    resp = self._sock_client.read_server_response(timeout=1)
  File "/root/miniconda3/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 285, in read_server_response
    data = self._read_packet_bytes(timeout=timeout)
  File "/root/miniconda3/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 270, in _read_packet_bytes
    raise SockClientClosedError
wandb.sdk.lib.sock_client.SockClientClosedError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 70, in message_loop
    msg = self._read_message()
  File "/root/miniconda3/lib/python3.10/site-packages/wandb/sdk/interface/router_sock.py", line 30, in _read_message
    raise MessageRouterClosedError from e
wandb.sdk.interface.router.MessageRouterClosedError
2025-03-02 10:11:32,709 INFO    MsgRouterThr:360778 [mailbox.py:close():115] Closing mailbox, abandoning 1 handles.
